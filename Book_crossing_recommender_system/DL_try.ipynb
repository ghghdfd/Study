{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, multiply,concatenate,Lambda\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Activation, Dropout,Reshape,Dot\n",
    "from tensorflow.keras.regularizers import l2,l1\n",
    "from tensorflow.keras.optimizers import SGD, Adamax,RMSprop,Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Location</th>\n",
       "      <th>Age</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Rating</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>183</td>\n",
       "      <td>portugal</td>\n",
       "      <td>27.0</td>\n",
       "      <td>8401422825</td>\n",
       "      <td>9</td>\n",
       "      <td>Fahrenheit 451</td>\n",
       "      <td>Ray Bradbury</td>\n",
       "      <td>1993</td>\n",
       "      <td>Plaza &amp;amp Janes Editores, S.A.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>183</td>\n",
       "      <td>portugal</td>\n",
       "      <td>27.0</td>\n",
       "      <td>8440630794</td>\n",
       "      <td>7</td>\n",
       "      <td>Cuentos del Planeta Tierra</td>\n",
       "      <td>Arthur C. Clarke</td>\n",
       "      <td>1993</td>\n",
       "      <td>Ediciones B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>183</td>\n",
       "      <td>portugal</td>\n",
       "      <td>27.0</td>\n",
       "      <td>8476409419</td>\n",
       "      <td>8</td>\n",
       "      <td>Estudios sobre el amor</td>\n",
       "      <td>Jose Ortega Y Gaset</td>\n",
       "      <td>2001</td>\n",
       "      <td>Downtown Book Center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>242</td>\n",
       "      <td>germany</td>\n",
       "      <td>37.0</td>\n",
       "      <td>3257203659</td>\n",
       "      <td>9</td>\n",
       "      <td>Der illustrierte Mann. Erz?짚hlungen.</td>\n",
       "      <td>Ray Bradbury</td>\n",
       "      <td>2002</td>\n",
       "      <td>Diogenes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>242</td>\n",
       "      <td>germany</td>\n",
       "      <td>37.0</td>\n",
       "      <td>3257207522</td>\n",
       "      <td>10</td>\n",
       "      <td>Der K?쨋nig in Gelb.</td>\n",
       "      <td>Raymond Chandler</td>\n",
       "      <td>1980</td>\n",
       "      <td>Diogenes Verlag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35548</th>\n",
       "      <td>278843</td>\n",
       "      <td>usa</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1561580880</td>\n",
       "      <td>7</td>\n",
       "      <td>Easy Guide to Sewing Skirts (Sewing Companion ...</td>\n",
       "      <td>Marcy Tilton</td>\n",
       "      <td>1995</td>\n",
       "      <td>Taunton Press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35549</th>\n",
       "      <td>278851</td>\n",
       "      <td>usa</td>\n",
       "      <td>33.0</td>\n",
       "      <td>067161746X</td>\n",
       "      <td>7</td>\n",
       "      <td>The Bachelor Home Companion: A Practical Guide...</td>\n",
       "      <td>P.J. O'Rourke</td>\n",
       "      <td>1987</td>\n",
       "      <td>Pocket Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35550</th>\n",
       "      <td>278851</td>\n",
       "      <td>usa</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1558531025</td>\n",
       "      <td>8</td>\n",
       "      <td>Life's Little Instruction Book (Life's Little ...</td>\n",
       "      <td>H. Jackson Brown</td>\n",
       "      <td>1991</td>\n",
       "      <td>Thomas Nelson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35551</th>\n",
       "      <td>278851</td>\n",
       "      <td>usa</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1569661057</td>\n",
       "      <td>10</td>\n",
       "      <td>Dallas Street Map Guide and Directory, 2000 Ed...</td>\n",
       "      <td>Mapsco</td>\n",
       "      <td>1999</td>\n",
       "      <td>American Map Corporation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35552</th>\n",
       "      <td>278851</td>\n",
       "      <td>usa</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1885071213</td>\n",
       "      <td>7</td>\n",
       "      <td>Highpoint Adventures, A Pocket Guide to the 50...</td>\n",
       "      <td>Charlie Winger</td>\n",
       "      <td>1999</td>\n",
       "      <td>Sequoia Publishing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35553 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       User-ID  Location   Age        ISBN  Book-Rating  \\\n",
       "0          183  portugal  27.0  8401422825            9   \n",
       "1          183  portugal  27.0  8440630794            7   \n",
       "2          183  portugal  27.0  8476409419            8   \n",
       "3          242   germany  37.0  3257203659            9   \n",
       "4          242   germany  37.0  3257207522           10   \n",
       "...        ...       ...   ...         ...          ...   \n",
       "35548   278843       usa  28.0  1561580880            7   \n",
       "35549   278851       usa  33.0  067161746X            7   \n",
       "35550   278851       usa  33.0  1558531025            8   \n",
       "35551   278851       usa  33.0  1569661057           10   \n",
       "35552   278851       usa  33.0  1885071213            7   \n",
       "\n",
       "                                              Book-Title          Book-Author  \\\n",
       "0                                         Fahrenheit 451         Ray Bradbury   \n",
       "1                             Cuentos del Planeta Tierra     Arthur C. Clarke   \n",
       "2                                 Estudios sobre el amor  Jose Ortega Y Gaset   \n",
       "3                   Der illustrierte Mann. Erz?짚hlungen.         Ray Bradbury   \n",
       "4                                    Der K?쨋nig in Gelb.     Raymond Chandler   \n",
       "...                                                  ...                  ...   \n",
       "35548  Easy Guide to Sewing Skirts (Sewing Companion ...         Marcy Tilton   \n",
       "35549  The Bachelor Home Companion: A Practical Guide...        P.J. O'Rourke   \n",
       "35550  Life's Little Instruction Book (Life's Little ...     H. Jackson Brown   \n",
       "35551  Dallas Street Map Guide and Directory, 2000 Ed...               Mapsco   \n",
       "35552  Highpoint Adventures, A Pocket Guide to the 50...       Charlie Winger   \n",
       "\n",
       "       Year-Of-Publication                        Publisher  \n",
       "0                     1993  Plaza &amp Janes Editores, S.A.  \n",
       "1                     1993                      Ediciones B  \n",
       "2                     2001             Downtown Book Center  \n",
       "3                     2002                         Diogenes  \n",
       "4                     1980                  Diogenes Verlag  \n",
       "...                    ...                              ...  \n",
       "35548                 1995                    Taunton Press  \n",
       "35549                 1987                     Pocket Books  \n",
       "35550                 1991                    Thomas Nelson  \n",
       "35551                 1999         American Map Corporation  \n",
       "35552                 1999               Sequoia Publishing  \n",
       "\n",
       "[35553 rows x 9 columns]"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('br_data.csv');data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Rating</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>totalRatingCount</th>\n",
       "      <th>Location</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276762</td>\n",
       "      <td>3453092007</td>\n",
       "      <td>8</td>\n",
       "      <td>Die zweite Haut.</td>\n",
       "      <td>2</td>\n",
       "      <td>germany</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35433</td>\n",
       "      <td>3453092007</td>\n",
       "      <td>8</td>\n",
       "      <td>Die zweite Haut.</td>\n",
       "      <td>2</td>\n",
       "      <td>switzerland</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>222488</td>\n",
       "      <td>3499121581</td>\n",
       "      <td>9</td>\n",
       "      <td>Rubinroter Dschungel.</td>\n",
       "      <td>6</td>\n",
       "      <td>germany</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>227176</td>\n",
       "      <td>3499121581</td>\n",
       "      <td>7</td>\n",
       "      <td>Rubinroter Dschungel.</td>\n",
       "      <td>6</td>\n",
       "      <td>germany</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>167166</td>\n",
       "      <td>3499121581</td>\n",
       "      <td>7</td>\n",
       "      <td>Rubinroter Dschungel.</td>\n",
       "      <td>6</td>\n",
       "      <td>germany</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18876</th>\n",
       "      <td>179423</td>\n",
       "      <td>1570429227</td>\n",
       "      <td>7</td>\n",
       "      <td>Roses Are Red</td>\n",
       "      <td>2</td>\n",
       "      <td>usa</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18877</th>\n",
       "      <td>179423</td>\n",
       "      <td>1570429693</td>\n",
       "      <td>10</td>\n",
       "      <td>Wish You Well</td>\n",
       "      <td>2</td>\n",
       "      <td>usa</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18878</th>\n",
       "      <td>195914</td>\n",
       "      <td>3453180224</td>\n",
       "      <td>7</td>\n",
       "      <td>Lucy Sullivan wird heiraten.</td>\n",
       "      <td>3</td>\n",
       "      <td>germany</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18879</th>\n",
       "      <td>195927</td>\n",
       "      <td>3462031791</td>\n",
       "      <td>9</td>\n",
       "      <td>Aimee und Jaguar. Eine Liebesgeschichte, Berli...</td>\n",
       "      <td>3</td>\n",
       "      <td>germany</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18880</th>\n",
       "      <td>203411</td>\n",
       "      <td>345370775</td>\n",
       "      <td>9</td>\n",
       "      <td>Jurassic Park</td>\n",
       "      <td>3</td>\n",
       "      <td>usa</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18881 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       User-ID        ISBN  Book-Rating  \\\n",
       "0       276762  3453092007            8   \n",
       "1        35433  3453092007            8   \n",
       "2       222488  3499121581            9   \n",
       "3       227176  3499121581            7   \n",
       "4       167166  3499121581            7   \n",
       "...        ...         ...          ...   \n",
       "18876   179423  1570429227            7   \n",
       "18877   179423  1570429693           10   \n",
       "18878   195914  3453180224            7   \n",
       "18879   195927  3462031791            9   \n",
       "18880   203411   345370775            9   \n",
       "\n",
       "                                              Book-Title  totalRatingCount  \\\n",
       "0                                       Die zweite Haut.                 2   \n",
       "1                                       Die zweite Haut.                 2   \n",
       "2                                  Rubinroter Dschungel.                 6   \n",
       "3                                  Rubinroter Dschungel.                 6   \n",
       "4                                  Rubinroter Dschungel.                 6   \n",
       "...                                                  ...               ...   \n",
       "18876                                      Roses Are Red                 2   \n",
       "18877                                      Wish You Well                 2   \n",
       "18878                       Lucy Sullivan wird heiraten.                 3   \n",
       "18879  Aimee und Jaguar. Eine Liebesgeschichte, Berli...                 3   \n",
       "18880                                      Jurassic Park                 3   \n",
       "\n",
       "          Location   Age  \n",
       "0          germany  25.0  \n",
       "1      switzerland  40.0  \n",
       "2          germany  29.0  \n",
       "3          germany  27.0  \n",
       "4          germany  44.0  \n",
       "...            ...   ...  \n",
       "18876          usa  32.0  \n",
       "18877          usa  32.0  \n",
       "18878      germany  23.0  \n",
       "18879      germany  45.0  \n",
       "18880          usa  26.0  \n",
       "\n",
       "[18881 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('combined_data.csv');data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:,'Age_c']=data['Age']//10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Age_c']=data['Age_c'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Age_c'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4198"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['User-ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=data.drop_duplicates(['ISBN']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=LabelEncoder()\n",
    "data['User-ID']=sc.fit_transform(data['User-ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=LabelEncoder()\n",
    "data['Location']=sc.fit_transform(data['Location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=LabelEncoder()\n",
    "data['Book-Author']=sc.fit_transform(data['Book-Author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=LabelEncoder()\n",
    "data['Publisher']=sc.fit_transform(data['Publisher'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=LabelEncoder()\n",
    "data['ISBN']=sc.fit_transform(data['ISBN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Age']=data['Age'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Rating</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>totalRatingCount</th>\n",
       "      <th>Location</th>\n",
       "      <th>Age</th>\n",
       "      <th>Age_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4165</td>\n",
       "      <td>4565</td>\n",
       "      <td>8</td>\n",
       "      <td>Die zweite Haut.</td>\n",
       "      <td>2</td>\n",
       "      <td>germany</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>498</td>\n",
       "      <td>4565</td>\n",
       "      <td>8</td>\n",
       "      <td>Die zweite Haut.</td>\n",
       "      <td>2</td>\n",
       "      <td>switzerland</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3324</td>\n",
       "      <td>4739</td>\n",
       "      <td>9</td>\n",
       "      <td>Rubinroter Dschungel.</td>\n",
       "      <td>6</td>\n",
       "      <td>germany</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3398</td>\n",
       "      <td>4739</td>\n",
       "      <td>7</td>\n",
       "      <td>Rubinroter Dschungel.</td>\n",
       "      <td>6</td>\n",
       "      <td>germany</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2496</td>\n",
       "      <td>4739</td>\n",
       "      <td>7</td>\n",
       "      <td>Rubinroter Dschungel.</td>\n",
       "      <td>6</td>\n",
       "      <td>germany</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18876</th>\n",
       "      <td>2699</td>\n",
       "      <td>2611</td>\n",
       "      <td>7</td>\n",
       "      <td>Roses Are Red</td>\n",
       "      <td>2</td>\n",
       "      <td>usa</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18877</th>\n",
       "      <td>2699</td>\n",
       "      <td>2613</td>\n",
       "      <td>10</td>\n",
       "      <td>Wish You Well</td>\n",
       "      <td>2</td>\n",
       "      <td>usa</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18878</th>\n",
       "      <td>2942</td>\n",
       "      <td>4626</td>\n",
       "      <td>7</td>\n",
       "      <td>Lucy Sullivan wird heiraten.</td>\n",
       "      <td>3</td>\n",
       "      <td>germany</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18879</th>\n",
       "      <td>2943</td>\n",
       "      <td>4678</td>\n",
       "      <td>9</td>\n",
       "      <td>Aimee und Jaguar. Eine Liebesgeschichte, Berli...</td>\n",
       "      <td>3</td>\n",
       "      <td>germany</td>\n",
       "      <td>45</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18880</th>\n",
       "      <td>3051</td>\n",
       "      <td>4649</td>\n",
       "      <td>9</td>\n",
       "      <td>Jurassic Park</td>\n",
       "      <td>3</td>\n",
       "      <td>usa</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18881 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       User-ID  ISBN  Book-Rating  \\\n",
       "0         4165  4565            8   \n",
       "1          498  4565            8   \n",
       "2         3324  4739            9   \n",
       "3         3398  4739            7   \n",
       "4         2496  4739            7   \n",
       "...        ...   ...          ...   \n",
       "18876     2699  2611            7   \n",
       "18877     2699  2613           10   \n",
       "18878     2942  4626            7   \n",
       "18879     2943  4678            9   \n",
       "18880     3051  4649            9   \n",
       "\n",
       "                                              Book-Title  totalRatingCount  \\\n",
       "0                                       Die zweite Haut.                 2   \n",
       "1                                       Die zweite Haut.                 2   \n",
       "2                                  Rubinroter Dschungel.                 6   \n",
       "3                                  Rubinroter Dschungel.                 6   \n",
       "4                                  Rubinroter Dschungel.                 6   \n",
       "...                                                  ...               ...   \n",
       "18876                                      Roses Are Red                 2   \n",
       "18877                                      Wish You Well                 2   \n",
       "18878                       Lucy Sullivan wird heiraten.                 3   \n",
       "18879  Aimee und Jaguar. Eine Liebesgeschichte, Berli...                 3   \n",
       "18880                                      Jurassic Park                 3   \n",
       "\n",
       "          Location  Age  Age_c  \n",
       "0          germany   25      2  \n",
       "1      switzerland   40      4  \n",
       "2          germany   29      2  \n",
       "3          germany   27      2  \n",
       "4          germany   44      4  \n",
       "...            ...  ...    ...  \n",
       "18876          usa   32      3  \n",
       "18877          usa   32      3  \n",
       "18878      germany   23      2  \n",
       "18879      germany   45      4  \n",
       "18880          usa   26      2  \n",
       "\n",
       "[18881 rows x 8 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.drop('Book-Rating',axis=1)\n",
    "\n",
    "Y = data['Book-Rating']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.849107568455061"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable 초기화 \n",
    "K = 30                           # Latent factor 수 \n",
    "reg = 0.05                      # Regularization penalty\n",
    "mu = y_train.mean()    # 전체 평균 \n",
    "M = data['User-ID'].max() + 1       # Number of users\n",
    "N = data['ISBN'].max() + 1 \n",
    "C = data['totalRatingCount'].max() + 1\n",
    "A = data['Age_c'].max() + 1\n",
    "L = data['Location'].max() + 1  # Number of movies\n",
    "# A = data['Book-Author'].max() + 1  # Number of movies\n",
    "# P = data['Publisher'].max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 10\n"
     ]
    }
   ],
   "source": [
    "min_rating = min(Y)\n",
    "max_rating = max(Y)\n",
    "\n",
    "print(min_rating,max_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining RMSE measure\n",
    "def RMSE(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = Input(shape=(1, )) # User input\n",
    "item = Input(shape=(1, ))                                               # Item input\n",
    "count = Input(shape=(1, )) \n",
    "age = Input(shape=(1, )) \n",
    "loc = Input(shape=(1, )) \n",
    "\n",
    "\n",
    "\n",
    "P_embedding = Embedding(M, K, embeddings_regularizer=l2(reg),embeddings_initializer='he_normal')(user)     # (M, 1, K)\n",
    "Q_embedding = Embedding(N, K, embeddings_regularizer=l2(reg),embeddings_initializer='he_normal')(item)     # (N, 1, K)\n",
    "C_embedding = Embedding(C, K, embeddings_regularizer=l2(reg),embeddings_initializer='he_normal')(count)\n",
    "A_embedding = Embedding(A, K, embeddings_regularizer=l2(reg),embeddings_initializer='he_normal')(age)\n",
    "L_embedding = Embedding(L, K, embeddings_regularizer=l2(reg),embeddings_initializer='he_normal')(loc)\n",
    "# P_embedding = Flatten()(P_embedding)                                    # (N, K)\n",
    "# Q_embedding = Flatten()(Q_embedding)  \n",
    "\n",
    "user_bias = Embedding(M, 1, embeddings_regularizer=l2(reg),embeddings_initializer='he_normal')(user)       # User bias term (M, 1, )\n",
    "item_bias = Embedding(N, 1, embeddings_regularizer=l2(reg),embeddings_initializer='he_normal')(item)       # Item bias term (N, 1, )\n",
    "count_bias = Embedding(C, 1, embeddings_regularizer=l2(reg),embeddings_initializer='he_normal')(count)\n",
    "age_bias = Embedding(A, 1, embeddings_regularizer=l2(reg),embeddings_initializer='he_normal')(age)\n",
    "loc_bias = Embedding(L, 1, embeddings_regularizer=l2(reg),embeddings_initializer='he_normal')(loc)\n",
    "# user_bias = Flatten()(user_bias)                                        # (N, K)\n",
    "# item_bias = Flatten()(item_bias)   \n",
    "\n",
    "R= multiply([P_embedding,Q_embedding,C_embedding,A_embedding,L_embedding])\n",
    "R= concatenate([R,user_bias,item_bias,count_bias,age_bias,loc_bias])\n",
    "# R = Activation('relu')(R)\n",
    "R = Lambda(lambda x: x * (max_rating - min_rating) + min_rating)(R)\n",
    "# R = Flatten()(R)\n",
    "# R = Dense(10, activation=\"relu\")(R)\n",
    "# R= Dropout(0.3)(R)\n",
    "R = Dense(1)(R)\n",
    "\n",
    "model = Model(inputs=[user, item,count,age,loc], outputs=R)\n",
    "model.compile(\n",
    "  loss='mean_squared_error',\n",
    "#    optimizer=SGD(lr=0.001, momentum=0.9),\n",
    "#     optimizer=RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0),\n",
    "#   optimizer=Adam(lr=0.001),\n",
    "   optimizer='adam',\n",
    "  metrics=[RMSE]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = Input(shape=(1, )) # User input\n",
    "item = Input(shape=(1, ))                                               # Item input\n",
    "count = Input(shape=(1, )) \n",
    "age = Input(shape=(1, )) \n",
    "loc = Input(shape=(1, )) \n",
    "\n",
    "\n",
    "\n",
    "P_embedding = Embedding(M, K, embeddings_regularizer=l2(reg),embeddings_initializer='he_normal')(user)     # (M, 1, K)\n",
    "Q_embedding = Embedding(N, K, embeddings_regularizer=l2(reg),embeddings_initializer='he_normal')(item)     # (N, 1, K)\n",
    "# C_embedding = Embedding(C, K, embeddings_regularizer=l2(reg),embeddings_initializer='he_normal')(count)\n",
    "# A_embedding = Embedding(A, K, embeddings_regularizer=l2(reg),embeddings_initializer='he_normal')(age)\n",
    "# L_embedding = Embedding(L, K, embeddings_regularizer=l2(reg),embeddings_initializer='he_normal')(loc)\n",
    "# P_embedding = Flatten()(P_embedding)                                    # (N, K)\n",
    "# Q_embedding = Flatten()(Q_embedding)  \n",
    "\n",
    "user_bias = Embedding(M, 1, embeddings_regularizer=l2(reg),embeddings_initializer='he_normal')(user)       # User bias term (M, 1, )\n",
    "item_bias = Embedding(N, 1, embeddings_regularizer=l2(reg),embeddings_initializer='he_normal')(item)       # Item bias term (N, 1, )\n",
    "# count_bias = Embedding(C, 1, embeddings_regularizer=l2(reg),embeddings_initializer='he_normal')(count)\n",
    "# age_bias = Embedding(A, 1, embeddings_regularizer=l2(reg),embeddings_initializer='he_normal')(age)\n",
    "# loc_bias = Embedding(L, 1, embeddings_regularizer=l2(reg),embeddings_initializer='he_normal')(loc)\n",
    "# user_bias = Flatten()(user_bias)                                        # (N, K)\n",
    "# item_bias = Flatten()(item_bias)   \n",
    "\n",
    "R= multiply([P_embedding,Q_embedding])\n",
    "R= concatenate([R,user_bias,item_bias])\n",
    "R = Lambda(lambda x: x * (max_rating - min_rating) - min_rating)(R)\n",
    "# R = Flatten()(R)\n",
    "# R = Dense(10, activation=\"relu\")(R)\n",
    "# R= Dropout(0.3)(R)\n",
    "R = Dense(1)(R)\n",
    "\n",
    "model = Model(inputs=[user, item], outputs=R)\n",
    "model.compile(\n",
    "  loss='mean_squared_error',\n",
    "#    optimizer=SGD(lr=0.001, momentum=0.9),\n",
    "#     optimizer=RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0),\n",
    "#   optimizer=Adam(lr=0.001),\n",
    "   optimizer='adam',\n",
    "  metrics=[RMSE]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_303\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_439 (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_440 (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_864 (Embedding)       (None, 1, 30)        125940      input_439[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_865 (Embedding)       (None, 1, 30)        166500      input_440[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_103 (Multiply)         (None, 1, 30)        0           embedding_864[0][0]              \n",
      "                                                                 embedding_865[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_866 (Embedding)       (None, 1, 1)         4198        input_439[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_867 (Embedding)       (None, 1, 1)         5550        input_440[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_134 (Concatenate)   (None, 1, 32)        0           multiply_103[0][0]               \n",
      "                                                                 embedding_866[0][0]              \n",
      "                                                                 embedding_867[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_49 (Lambda)              (None, 1, 32)        0           concatenate_134[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_243 (Dense)               (None, 1, 1)         33          lambda_49[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 302,221\n",
      "Trainable params: 302,221\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_RMSE', mode='min', verbose=1, patience=5)\n",
    "mc = ModelCheckpoint('best_model_multiply.h5', monitor='val_RMSE', mode='min', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "104/104 [==============================] - 0s 3ms/step - loss: 47.3356 - RMSE: 6.7635 - val_loss: 28.2674 - val_RMSE: 5.3067\n",
      "Epoch 2/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 17.7218 - RMSE: 4.1416 - val_loss: 10.5024 - val_RMSE: 3.2115\n",
      "Epoch 3/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 7.0477 - RMSE: 2.5988 - val_loss: 4.8760 - val_RMSE: 2.1626\n",
      "Epoch 4/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 4.0782 - RMSE: 1.9713 - val_loss: 3.5061 - val_RMSE: 1.8269\n",
      "Epoch 5/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 3.4428 - RMSE: 1.8114 - val_loss: 3.2208 - val_RMSE: 1.7567\n",
      "Epoch 6/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 3.3145 - RMSE: 1.7822 - val_loss: 3.1483 - val_RMSE: 1.7423\n",
      "Epoch 7/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 3.2776 - RMSE: 1.7789 - val_loss: 3.1223 - val_RMSE: 1.7386\n",
      "Epoch 8/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 3.2603 - RMSE: 1.7755 - val_loss: 3.1111 - val_RMSE: 1.7371\n",
      "Epoch 9/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 3.2492 - RMSE: 1.7765 - val_loss: 3.1023 - val_RMSE: 1.7351\n",
      "Epoch 10/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 3.2413 - RMSE: 1.7742 - val_loss: 3.0975 - val_RMSE: 1.7336\n",
      "Epoch 11/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 3.2348 - RMSE: 1.7723 - val_loss: 3.0959 - val_RMSE: 1.7325\n",
      "Epoch 12/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 3.2288 - RMSE: 1.7671 - val_loss: 3.0935 - val_RMSE: 1.7308\n",
      "Epoch 13/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 3.2228 - RMSE: 1.7666 - val_loss: 3.0905 - val_RMSE: 1.7286\n",
      "Epoch 14/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 3.2162 - RMSE: 1.7626 - val_loss: 3.0892 - val_RMSE: 1.7267\n",
      "Epoch 15/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 3.2090 - RMSE: 1.7588 - val_loss: 3.0877 - val_RMSE: 1.7244\n",
      "Epoch 16/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 3.2011 - RMSE: 1.7563 - val_loss: 3.0835 - val_RMSE: 1.7213\n",
      "Epoch 17/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 3.1923 - RMSE: 1.7472 - val_loss: 3.0809 - val_RMSE: 1.7182\n",
      "Epoch 18/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 3.1820 - RMSE: 1.7462 - val_loss: 3.0787 - val_RMSE: 1.7151\n",
      "Epoch 19/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 3.1706 - RMSE: 1.7439 - val_loss: 3.0751 - val_RMSE: 1.7111\n",
      "Epoch 20/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 3.1588 - RMSE: 1.7334 - val_loss: 3.0712 - val_RMSE: 1.7069\n",
      "Epoch 21/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 3.1436 - RMSE: 1.7261 - val_loss: 3.0707 - val_RMSE: 1.7037\n",
      "Epoch 22/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 3.1276 - RMSE: 1.7153 - val_loss: 3.0654 - val_RMSE: 1.6983\n",
      "Epoch 23/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 3.1089 - RMSE: 1.7056 - val_loss: 3.0608 - val_RMSE: 1.6931\n",
      "Epoch 24/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 3.0903 - RMSE: 1.7001 - val_loss: 3.0578 - val_RMSE: 1.6883\n",
      "Epoch 25/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 3.0692 - RMSE: 1.6856 - val_loss: 3.0528 - val_RMSE: 1.6821\n",
      "Epoch 26/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 3.0455 - RMSE: 1.6757 - val_loss: 3.0464 - val_RMSE: 1.6760\n",
      "Epoch 27/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 3.0208 - RMSE: 1.6623 - val_loss: 3.0440 - val_RMSE: 1.6707\n",
      "Epoch 28/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.9949 - RMSE: 1.6508 - val_loss: 3.0353 - val_RMSE: 1.6636\n",
      "Epoch 29/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.9681 - RMSE: 1.6390 - val_loss: 3.0336 - val_RMSE: 1.6587\n",
      "Epoch 30/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.9389 - RMSE: 1.6268 - val_loss: 3.0292 - val_RMSE: 1.6528\n",
      "Epoch 31/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.9085 - RMSE: 1.6096 - val_loss: 3.0235 - val_RMSE: 1.6466\n",
      "Epoch 32/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.8782 - RMSE: 1.5964 - val_loss: 3.0209 - val_RMSE: 1.6411\n",
      "Epoch 33/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.8459 - RMSE: 1.5833 - val_loss: 3.0184 - val_RMSE: 1.6361\n",
      "Epoch 34/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.8143 - RMSE: 1.5709 - val_loss: 3.0132 - val_RMSE: 1.6302\n",
      "Epoch 35/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.7795 - RMSE: 1.5546 - val_loss: 3.0146 - val_RMSE: 1.6263\n",
      "Epoch 36/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.7478 - RMSE: 1.5418 - val_loss: 3.0173 - val_RMSE: 1.6232\n",
      "Epoch 37/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.7139 - RMSE: 1.5273 - val_loss: 3.0089 - val_RMSE: 1.6164\n",
      "Epoch 38/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.6811 - RMSE: 1.5109 - val_loss: 3.0086 - val_RMSE: 1.6129\n",
      "Epoch 39/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.6472 - RMSE: 1.4945 - val_loss: 3.0139 - val_RMSE: 1.6110\n",
      "Epoch 40/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.6132 - RMSE: 1.4812 - val_loss: 3.0122 - val_RMSE: 1.6072\n",
      "Epoch 41/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.5805 - RMSE: 1.4655 - val_loss: 3.0156 - val_RMSE: 1.6055\n",
      "Epoch 42/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.5480 - RMSE: 1.4488 - val_loss: 3.0174 - val_RMSE: 1.6030\n",
      "Epoch 43/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.5146 - RMSE: 1.4346 - val_loss: 3.0162 - val_RMSE: 1.6004\n",
      "Epoch 44/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.4815 - RMSE: 1.4207 - val_loss: 3.0276 - val_RMSE: 1.6017\n",
      "Epoch 45/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.4501 - RMSE: 1.4077 - val_loss: 3.0282 - val_RMSE: 1.5992\n",
      "Epoch 46/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.4196 - RMSE: 1.3926 - val_loss: 3.0282 - val_RMSE: 1.5975\n",
      "Epoch 47/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.3869 - RMSE: 1.3792 - val_loss: 3.0341 - val_RMSE: 1.5970\n",
      "Epoch 48/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.3573 - RMSE: 1.3649 - val_loss: 3.0426 - val_RMSE: 1.5974\n",
      "Epoch 49/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.3255 - RMSE: 1.3522 - val_loss: 3.0473 - val_RMSE: 1.5981\n",
      "Epoch 50/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.2981 - RMSE: 1.3398 - val_loss: 3.0476 - val_RMSE: 1.5960\n",
      "Epoch 51/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.2684 - RMSE: 1.3262 - val_loss: 3.0575 - val_RMSE: 1.5983\n",
      "Epoch 52/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.2416 - RMSE: 1.3160 - val_loss: 3.0603 - val_RMSE: 1.5977\n",
      "Epoch 53/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.2126 - RMSE: 1.3028 - val_loss: 3.0682 - val_RMSE: 1.6002\n",
      "Epoch 54/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.1840 - RMSE: 1.2905 - val_loss: 3.0758 - val_RMSE: 1.6012\n",
      "Epoch 55/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.1579 - RMSE: 1.2833 - val_loss: 3.0740 - val_RMSE: 1.5999\n",
      "Epoch 56/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.1321 - RMSE: 1.2690 - val_loss: 3.0843 - val_RMSE: 1.6027\n",
      "Epoch 57/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.1075 - RMSE: 1.2597 - val_loss: 3.0901 - val_RMSE: 1.6041\n",
      "Epoch 58/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.0825 - RMSE: 1.2494 - val_loss: 3.1091 - val_RMSE: 1.6101\n",
      "Epoch 59/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.0595 - RMSE: 1.2376 - val_loss: 3.1038 - val_RMSE: 1.6080\n",
      "Epoch 60/60\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 2.0376 - RMSE: 1.2322 - val_loss: 3.1197 - val_RMSE: 1.6128\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=[x_train['User-ID'].values, x_train['ISBN'].values],y=y_train, batch_size= 128, epochs=60, \n",
    "                    verbose= 1, validation_data=([x_test['User-ID'].values, x_test['ISBN'].values], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gommg\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 6ms/step - loss: 12.6553 - RMSE: 1.8747 - val_loss: 9.5091 - val_RMSE: 1.7858\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 8.6612 - RMSE: 1.8397 - val_loss: 7.4814 - val_RMSE: 1.7681\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 7.0383 - RMSE: 1.8250 - val_loss: 6.2229 - val_RMSE: 1.7603\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 5.9970 - RMSE: 1.8149 - val_loss: 5.4010 - val_RMSE: 1.7573\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 5.3006 - RMSE: 1.8109 - val_loss: 4.8350 - val_RMSE: 1.7537\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 4.8139 - RMSE: 1.8056 - val_loss: 4.4252 - val_RMSE: 1.7477\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 4.4605 - RMSE: 1.7992 - val_loss: 4.1238 - val_RMSE: 1.7417\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 4.1934 - RMSE: 1.7943 - val_loss: 3.8965 - val_RMSE: 1.7354\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 3.9891 - RMSE: 1.7847 - val_loss: 3.7291 - val_RMSE: 1.7310\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 3.8132 - RMSE: 1.7736 - val_loss: 3.5773 - val_RMSE: 1.7194\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 3.6692 - RMSE: 1.7593 - val_loss: 3.4704 - val_RMSE: 1.7128\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 3.5430 - RMSE: 1.7431 - val_loss: 3.3672 - val_RMSE: 1.6999\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 3.4338 - RMSE: 1.7273 - val_loss: 3.2920 - val_RMSE: 1.6908\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 3.3347 - RMSE: 1.7116 - val_loss: 3.2282 - val_RMSE: 1.6812\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 3.2458 - RMSE: 1.6940 - val_loss: 3.1648 - val_RMSE: 1.6683\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 3.1664 - RMSE: 1.6742 - val_loss: 3.1250 - val_RMSE: 1.6606\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 3.0953 - RMSE: 1.6558 - val_loss: 3.0857 - val_RMSE: 1.6511\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 3.0292 - RMSE: 1.6396 - val_loss: 3.0629 - val_RMSE: 1.6461\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.9676 - RMSE: 1.6200 - val_loss: 3.0326 - val_RMSE: 1.6371\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.9145 - RMSE: 1.6048 - val_loss: 3.0138 - val_RMSE: 1.6310\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.8622 - RMSE: 1.5856 - val_loss: 3.0008 - val_RMSE: 1.6256\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.8135 - RMSE: 1.5695 - val_loss: 2.9984 - val_RMSE: 1.6243\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.7673 - RMSE: 1.5532 - val_loss: 2.9794 - val_RMSE: 1.6159\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.7295 - RMSE: 1.5393 - val_loss: 2.9748 - val_RMSE: 1.6120\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.6817 - RMSE: 1.5203 - val_loss: 2.9726 - val_RMSE: 1.6088\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.6450 - RMSE: 1.5061 - val_loss: 2.9722 - val_RMSE: 1.6067\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.6073 - RMSE: 1.4900 - val_loss: 2.9788 - val_RMSE: 1.6068\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.5694 - RMSE: 1.4730 - val_loss: 2.9749 - val_RMSE: 1.6029\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.5355 - RMSE: 1.4602 - val_loss: 2.9737 - val_RMSE: 1.5996\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.5023 - RMSE: 1.4459 - val_loss: 2.9739 - val_RMSE: 1.5974\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.4686 - RMSE: 1.4309 - val_loss: 2.9927 - val_RMSE: 1.6022\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.4383 - RMSE: 1.4180 - val_loss: 2.9785 - val_RMSE: 1.5939\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.4092 - RMSE: 1.4046 - val_loss: 2.9851 - val_RMSE: 1.5939\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.3751 - RMSE: 1.3894 - val_loss: 2.9902 - val_RMSE: 1.5934\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.3461 - RMSE: 1.3775 - val_loss: 2.9971 - val_RMSE: 1.5931\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.3214 - RMSE: 1.3644 - val_loss: 2.9990 - val_RMSE: 1.5925\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.2950 - RMSE: 1.3531 - val_loss: 3.0088 - val_RMSE: 1.5945\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.2663 - RMSE: 1.3413 - val_loss: 3.0051 - val_RMSE: 1.5920\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.2409 - RMSE: 1.3290 - val_loss: 3.0185 - val_RMSE: 1.5950\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.2171 - RMSE: 1.3176 - val_loss: 3.0216 - val_RMSE: 1.5940\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.1913 - RMSE: 1.3063 - val_loss: 3.0244 - val_RMSE: 1.5937\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.1699 - RMSE: 1.2983 - val_loss: 3.0403 - val_RMSE: 1.5992\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.1473 - RMSE: 1.2870 - val_loss: 3.0412 - val_RMSE: 1.5979\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.1262 - RMSE: 1.2781 - val_loss: 3.0436 - val_RMSE: 1.5979\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.1029 - RMSE: 1.2683 - val_loss: 3.0479 - val_RMSE: 1.5987\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.0843 - RMSE: 1.2605 - val_loss: 3.0519 - val_RMSE: 1.5993\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.0629 - RMSE: 1.2509 - val_loss: 3.0604 - val_RMSE: 1.6010\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.0433 - RMSE: 1.2429 - val_loss: 3.0642 - val_RMSE: 1.6028\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.0267 - RMSE: 1.2367 - val_loss: 3.0698 - val_RMSE: 1.6041\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.0084 - RMSE: 1.2273 - val_loss: 3.0765 - val_RMSE: 1.6050\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.9918 - RMSE: 1.2198 - val_loss: 3.0840 - val_RMSE: 1.6080\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.9737 - RMSE: 1.2131 - val_loss: 3.0875 - val_RMSE: 1.6084\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.9562 - RMSE: 1.2054 - val_loss: 3.0945 - val_RMSE: 1.6109\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.9404 - RMSE: 1.1989 - val_loss: 3.0994 - val_RMSE: 1.6128\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.9258 - RMSE: 1.1931 - val_loss: 3.0996 - val_RMSE: 1.6129\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.9107 - RMSE: 1.1868 - val_loss: 3.1106 - val_RMSE: 1.6163\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1.8958 - RMSE: 1.1809 - val_loss: 3.1081 - val_RMSE: 1.6162\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.8801 - RMSE: 1.1739 - val_loss: 3.1153 - val_RMSE: 1.6184\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.8684 - RMSE: 1.1694 - val_loss: 3.1237 - val_RMSE: 1.6218\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1.8546 - RMSE: 1.1639 - val_loss: 3.1279 - val_RMSE: 1.6227\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.8417 - RMSE: 1.1582 - val_loss: 3.1334 - val_RMSE: 1.6246\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.8286 - RMSE: 1.1533 - val_loss: 3.1419 - val_RMSE: 1.6286\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.8161 - RMSE: 1.1488 - val_loss: 3.1458 - val_RMSE: 1.6299\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.8034 - RMSE: 1.1424 - val_loss: 3.1473 - val_RMSE: 1.6307\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.7919 - RMSE: 1.1384 - val_loss: 3.1552 - val_RMSE: 1.6342\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.7796 - RMSE: 1.1344 - val_loss: 3.1541 - val_RMSE: 1.6343\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1.7683 - RMSE: 1.1298 - val_loss: 3.1664 - val_RMSE: 1.6381\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.7572 - RMSE: 1.1266 - val_loss: 3.1713 - val_RMSE: 1.6402\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.7479 - RMSE: 1.1226 - val_loss: 3.1754 - val_RMSE: 1.6421\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.7368 - RMSE: 1.1182 - val_loss: 3.1783 - val_RMSE: 1.6433\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.7299 - RMSE: 1.1157 - val_loss: 3.1878 - val_RMSE: 1.6468\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.7144 - RMSE: 1.1098 - val_loss: 3.1852 - val_RMSE: 1.6477\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.7072 - RMSE: 1.1071 - val_loss: 3.1971 - val_RMSE: 1.6515\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.6993 - RMSE: 1.1046 - val_loss: 3.1950 - val_RMSE: 1.6513\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.6891 - RMSE: 1.1007 - val_loss: 3.2037 - val_RMSE: 1.6545\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.6791 - RMSE: 1.0983 - val_loss: 3.2094 - val_RMSE: 1.6566\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.6717 - RMSE: 1.0948 - val_loss: 3.2124 - val_RMSE: 1.6578\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.6618 - RMSE: 1.0911 - val_loss: 3.2162 - val_RMSE: 1.6606\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.6542 - RMSE: 1.0888 - val_loss: 3.2212 - val_RMSE: 1.6614\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.6449 - RMSE: 1.0857 - val_loss: 3.2261 - val_RMSE: 1.6646\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.6376 - RMSE: 1.0830 - val_loss: 3.2240 - val_RMSE: 1.6632\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.6298 - RMSE: 1.0792 - val_loss: 3.2371 - val_RMSE: 1.6683\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.6228 - RMSE: 1.0771 - val_loss: 3.2401 - val_RMSE: 1.6694\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1.6131 - RMSE: 1.0755 - val_loss: 3.2483 - val_RMSE: 1.6735\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.6078 - RMSE: 1.0726 - val_loss: 3.2490 - val_RMSE: 1.6738\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.6015 - RMSE: 1.0701 - val_loss: 3.2541 - val_RMSE: 1.6754\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.5928 - RMSE: 1.0659 - val_loss: 3.2614 - val_RMSE: 1.6789\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.5864 - RMSE: 1.0656 - val_loss: 3.2666 - val_RMSE: 1.6816\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.5800 - RMSE: 1.0633 - val_loss: 3.2680 - val_RMSE: 1.6829\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.5729 - RMSE: 1.0600 - val_loss: 3.2718 - val_RMSE: 1.6844\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.5664 - RMSE: 1.0573 - val_loss: 3.2782 - val_RMSE: 1.6870\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.5602 - RMSE: 1.0571 - val_loss: 3.2808 - val_RMSE: 1.6879\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.5559 - RMSE: 1.0565 - val_loss: 3.2852 - val_RMSE: 1.6898\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.5485 - RMSE: 1.0528 - val_loss: 3.2977 - val_RMSE: 1.6948\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.5437 - RMSE: 1.0504 - val_loss: 3.2942 - val_RMSE: 1.6933\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.5375 - RMSE: 1.0508 - val_loss: 3.3032 - val_RMSE: 1.6973\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.5311 - RMSE: 1.0466 - val_loss: 3.3033 - val_RMSE: 1.6979\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.5236 - RMSE: 1.0451 - val_loss: 3.3074 - val_RMSE: 1.6981\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.5194 - RMSE: 1.0437 - val_loss: 3.3166 - val_RMSE: 1.7041\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.5156 - RMSE: 1.0427 - val_loss: 3.3192 - val_RMSE: 1.7024\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=[x_train['User-ID'].values, x_train['ISBN'].values,x_train['totalRatingCount'].values,x_train['Age_c'].values,x_train['Location'].values],y=y_train-mu, batch_size= batch_size, epochs=100, \n",
    "                    verbose= 1, validation_data=([x_test['User-ID'].values, x_test['ISBN'].values,x_test['totalRatingCount'].values,x_test['Age_c'].values,x_test['Location'].values], y_test-mu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 59.6609 - RMSE: 7.0078 - val_loss: 46.5119 - val_RMSE: 6.2754\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 37.8430 - RMSE: 5.5994 - val_loss: 29.8614 - val_RMSE: 4.9520\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 24.0295 - RMSE: 4.3822 - val_loss: 18.9955 - val_RMSE: 3.8490\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 15.3096 - RMSE: 3.3821 - val_loss: 12.4322 - val_RMSE: 3.0137\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 10.2929 - RMSE: 2.6956 - val_loss: 8.8553 - val_RMSE: 2.4749\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 7.6350 - RMSE: 2.2655 - val_loss: 7.0044 - val_RMSE: 2.1790\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 6.2639 - RMSE: 2.0442 - val_loss: 6.0191 - val_RMSE: 2.0325\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 5.5013 - RMSE: 1.9406 - val_loss: 5.4413 - val_RMSE: 1.9604\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 5.0174 - RMSE: 1.8722 - val_loss: 5.0464 - val_RMSE: 1.9178\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 4.6670 - RMSE: 1.8320 - val_loss: 4.7502 - val_RMSE: 1.8880\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 4.3954 - RMSE: 1.8086 - val_loss: 4.5167 - val_RMSE: 1.8650\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 4.1785 - RMSE: 1.7807 - val_loss: 4.3289 - val_RMSE: 1.8465\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 4.0013 - RMSE: 1.7646 - val_loss: 4.1745 - val_RMSE: 1.8312\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.8549 - RMSE: 1.7445 - val_loss: 4.0481 - val_RMSE: 1.8185\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.7341 - RMSE: 1.7379 - val_loss: 3.9425 - val_RMSE: 1.8078\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.6322 - RMSE: 1.7260 - val_loss: 3.8545 - val_RMSE: 1.7989\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.5464 - RMSE: 1.7139 - val_loss: 3.7827 - val_RMSE: 1.7918\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.4731 - RMSE: 1.7028 - val_loss: 3.7201 - val_RMSE: 1.7851\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.4103 - RMSE: 1.7110 - val_loss: 3.6669 - val_RMSE: 1.7794\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.3571 - RMSE: 1.6869 - val_loss: 3.6218 - val_RMSE: 1.7745\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.3102 - RMSE: 1.6901 - val_loss: 3.5836 - val_RMSE: 1.7701\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.2692 - RMSE: 1.6838 - val_loss: 3.5499 - val_RMSE: 1.7661\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.2336 - RMSE: 1.6757 - val_loss: 3.5217 - val_RMSE: 1.7627\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.2024 - RMSE: 1.6684 - val_loss: 3.4956 - val_RMSE: 1.7592\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.1739 - RMSE: 1.6700 - val_loss: 3.4754 - val_RMSE: 1.7568\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.1488 - RMSE: 1.6571 - val_loss: 3.4576 - val_RMSE: 1.7544\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.1261 - RMSE: 1.6561 - val_loss: 3.4423 - val_RMSE: 1.7522\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.1056 - RMSE: 1.6593 - val_loss: 3.4295 - val_RMSE: 1.7504\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.0868 - RMSE: 1.6461 - val_loss: 3.4170 - val_RMSE: 1.7485\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.0700 - RMSE: 1.6462 - val_loss: 3.4059 - val_RMSE: 1.7465\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.0540 - RMSE: 1.6379 - val_loss: 3.3968 - val_RMSE: 1.7449\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 3.0388 - RMSE: 1.6415 - val_loss: 3.3886 - val_RMSE: 1.7431\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.0245 - RMSE: 1.6357 - val_loss: 3.3818 - val_RMSE: 1.7417\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.0113 - RMSE: 1.6244 - val_loss: 3.3750 - val_RMSE: 1.7401\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.9987 - RMSE: 1.6231 - val_loss: 3.3704 - val_RMSE: 1.7391\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.9867 - RMSE: 1.6143 - val_loss: 3.3652 - val_RMSE: 1.7376\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.9748 - RMSE: 1.6136 - val_loss: 3.3606 - val_RMSE: 1.7365\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.9631 - RMSE: 1.6140 - val_loss: 3.3569 - val_RMSE: 1.7352\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.9527 - RMSE: 1.6123 - val_loss: 3.3519 - val_RMSE: 1.7338\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.9408 - RMSE: 1.6041 - val_loss: 3.3470 - val_RMSE: 1.7322\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.9307 - RMSE: 1.5930 - val_loss: 3.3462 - val_RMSE: 1.7317\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.9197 - RMSE: 1.5934 - val_loss: 3.3446 - val_RMSE: 1.7309\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.9091 - RMSE: 1.5995 - val_loss: 3.3415 - val_RMSE: 1.7296\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.8991 - RMSE: 1.5937 - val_loss: 3.3402 - val_RMSE: 1.7286\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.8888 - RMSE: 1.5928 - val_loss: 3.3381 - val_RMSE: 1.7277\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.8779 - RMSE: 1.5803 - val_loss: 3.3350 - val_RMSE: 1.7265\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.8677 - RMSE: 1.5734 - val_loss: 3.3323 - val_RMSE: 1.7251\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.8571 - RMSE: 1.5789 - val_loss: 3.3308 - val_RMSE: 1.7241\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.8466 - RMSE: 1.5710 - val_loss: 3.3293 - val_RMSE: 1.7228\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.8361 - RMSE: 1.5736 - val_loss: 3.3275 - val_RMSE: 1.7219\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.8251 - RMSE: 1.5671 - val_loss: 3.3258 - val_RMSE: 1.7207\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.8156 - RMSE: 1.5704 - val_loss: 3.3242 - val_RMSE: 1.7194\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.8050 - RMSE: 1.5572 - val_loss: 3.3235 - val_RMSE: 1.7188\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.7941 - RMSE: 1.5476 - val_loss: 3.3234 - val_RMSE: 1.7180\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.7835 - RMSE: 1.5500 - val_loss: 3.3227 - val_RMSE: 1.7172\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.7729 - RMSE: 1.5453 - val_loss: 3.3239 - val_RMSE: 1.7165\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.7620 - RMSE: 1.5407 - val_loss: 3.3214 - val_RMSE: 1.7151\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.7511 - RMSE: 1.5299 - val_loss: 3.3221 - val_RMSE: 1.7145\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.7394 - RMSE: 1.5256 - val_loss: 3.3217 - val_RMSE: 1.7138\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.7284 - RMSE: 1.5214 - val_loss: 3.3218 - val_RMSE: 1.7131\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.7177 - RMSE: 1.5278 - val_loss: 3.3200 - val_RMSE: 1.7118\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.7065 - RMSE: 1.5159 - val_loss: 3.3239 - val_RMSE: 1.7120\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.6956 - RMSE: 1.5173 - val_loss: 3.3233 - val_RMSE: 1.7112\n",
      "Epoch 64/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.6843 - RMSE: 1.5073 - val_loss: 3.3250 - val_RMSE: 1.7106\n",
      "Epoch 65/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.6729 - RMSE: 1.4947 - val_loss: 3.3262 - val_RMSE: 1.7105\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.6616 - RMSE: 1.4922 - val_loss: 3.3260 - val_RMSE: 1.7099\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.6498 - RMSE: 1.4917 - val_loss: 3.3254 - val_RMSE: 1.7091\n",
      "Epoch 68/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.6393 - RMSE: 1.4871 - val_loss: 3.3256 - val_RMSE: 1.7084\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.6276 - RMSE: 1.4782 - val_loss: 3.3290 - val_RMSE: 1.7084\n",
      "Epoch 70/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.6163 - RMSE: 1.4794 - val_loss: 3.3321 - val_RMSE: 1.7085\n",
      "Epoch 71/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.6042 - RMSE: 1.4686 - val_loss: 3.3303 - val_RMSE: 1.7075\n",
      "Epoch 72/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.5927 - RMSE: 1.4708 - val_loss: 3.3303 - val_RMSE: 1.7070\n",
      "Epoch 73/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.5811 - RMSE: 1.4659 - val_loss: 3.3305 - val_RMSE: 1.7067\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.5690 - RMSE: 1.4676 - val_loss: 3.3335 - val_RMSE: 1.7065\n",
      "Epoch 75/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.5585 - RMSE: 1.4454 - val_loss: 3.3385 - val_RMSE: 1.7070\n",
      "Epoch 76/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.5474 - RMSE: 1.4566 - val_loss: 3.3378 - val_RMSE: 1.7061\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.5338 - RMSE: 1.4520 - val_loss: 3.3374 - val_RMSE: 1.7052\n",
      "Epoch 78/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.5224 - RMSE: 1.4387 - val_loss: 3.3421 - val_RMSE: 1.7054\n",
      "Epoch 79/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.5111 - RMSE: 1.4345 - val_loss: 3.3447 - val_RMSE: 1.7058\n",
      "Epoch 80/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.4989 - RMSE: 1.4281 - val_loss: 3.3478 - val_RMSE: 1.7060\n",
      "Epoch 81/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.4875 - RMSE: 1.4233 - val_loss: 3.3468 - val_RMSE: 1.7046\n",
      "Epoch 82/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.4741 - RMSE: 1.4169 - val_loss: 3.3469 - val_RMSE: 1.7038\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.4642 - RMSE: 1.4125 - val_loss: 3.3472 - val_RMSE: 1.7034\n",
      "Epoch 84/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.4503 - RMSE: 1.4112 - val_loss: 3.3492 - val_RMSE: 1.7032\n",
      "Epoch 85/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.4387 - RMSE: 1.3988 - val_loss: 3.3530 - val_RMSE: 1.7032\n",
      "Epoch 86/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.4257 - RMSE: 1.3977 - val_loss: 3.3557 - val_RMSE: 1.7030\n",
      "Epoch 87/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.4136 - RMSE: 1.3901 - val_loss: 3.3549 - val_RMSE: 1.7025\n",
      "Epoch 88/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.4008 - RMSE: 1.3965 - val_loss: 3.3544 - val_RMSE: 1.7017\n",
      "Epoch 89/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.3885 - RMSE: 1.3937 - val_loss: 3.3565 - val_RMSE: 1.7014\n",
      "Epoch 90/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.3758 - RMSE: 1.3729 - val_loss: 3.3606 - val_RMSE: 1.7015\n",
      "Epoch 91/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.3633 - RMSE: 1.3666 - val_loss: 3.3655 - val_RMSE: 1.7020\n",
      "Epoch 92/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.3503 - RMSE: 1.3653 - val_loss: 3.3688 - val_RMSE: 1.7022\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.3379 - RMSE: 1.3670 - val_loss: 3.3679 - val_RMSE: 1.7015\n",
      "Epoch 94/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.3264 - RMSE: 1.3529 - val_loss: 3.3722 - val_RMSE: 1.7017\n",
      "Epoch 95/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.3129 - RMSE: 1.3526 - val_loss: 3.3757 - val_RMSE: 1.7024\n",
      "Epoch 96/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.3009 - RMSE: 1.3407 - val_loss: 3.3788 - val_RMSE: 1.7025\n",
      "Epoch 97/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.2880 - RMSE: 1.3394 - val_loss: 3.3820 - val_RMSE: 1.7027\n",
      "Epoch 98/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.2750 - RMSE: 1.3283 - val_loss: 3.3828 - val_RMSE: 1.7024\n",
      "Epoch 99/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.2623 - RMSE: 1.3262 - val_loss: 3.3874 - val_RMSE: 1.7030\n",
      "Epoch 100/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.2501 - RMSE: 1.3259 - val_loss: 3.3924 - val_RMSE: 1.7039\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=[x_train['User-ID'].values, x_train['ISBN'].values,x_train['totalRatingCount'].values,x_train['Age_c'].values,x_train['Location'].values],y=y_train, batch_size= batch_size, epochs=100, \n",
    "                    verbose= 1, validation_split=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
